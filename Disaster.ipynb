{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "yGEKMv75cs14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading test data"
      ],
      "metadata": {
        "id": "iZCd5c_v4jYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/train.csv')"
      ],
      "metadata": {
        "id": "0Rh8Wv8nct-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filling missing values"
      ],
      "metadata": {
        "id": "e-7ZUDY54tED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = train.isnull().sum()\n",
        "print(missing_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AevJKd_dZIMP",
        "outputId": "6954ca14-b596-462b-9393-2a772249d251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id             0\n",
            "keyword       61\n",
            "place       2533\n",
            "tweet          0\n",
            "disaster       0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['keyword'].fillna('missing_keyword', inplace=True)\n",
        "train['place'].fillna('unknown', inplace=True)"
      ],
      "metadata": {
        "id": "zX8Nn0T4c0Ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = train.isnull().sum()\n",
        "print(missing_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJRC71ry1cxT",
        "outputId": "13b206b0-d37d-4cfd-e896-cb3a20ae1eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id          0\n",
            "keyword     0\n",
            "place       0\n",
            "tweet       0\n",
            "disaster    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define stopwords and preprocessing functions"
      ],
      "metadata": {
        "id": "JI7UpxYh4zHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
        "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
        "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
        "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
        "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
        "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
        "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
        "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
        "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
        "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
        "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
        "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
        "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
        "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
        "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
      ],
      "metadata": {
        "id": "wqVb0OGy1pIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS = set(stopwordlist)"
      ],
      "metadata": {
        "id": "YMZ3MGbU1qmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaning_stopwords(tweet):\n",
        "    return \" \".join([word for word in str(tweet).split() if word not in STOPWORDS])"
      ],
      "metadata": {
        "id": "wFaHNlaM18Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['tweet'].iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Hh7M9bws19g5",
        "outputId": "44ca12fb-9452-47a9-e51b-afa7472b062d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def cleaning_punctuations(tweet):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return tweet.translate(translator)"
      ],
      "metadata": {
        "id": "gqpDq7Sm2DmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def cleaning_repeating_char(tweet):\n",
        "    return re.sub(r'(.)\\1+', r'\\1', tweet)"
      ],
      "metadata": {
        "id": "DZ-bAGJy2Ixz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaning_URLs(tweet):\n",
        "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', ' ', tweet)"
      ],
      "metadata": {
        "id": "Qu1Yif742UT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaning_numbers(tweet):\n",
        "    return re.sub('[0-9]+', '', tweet)"
      ],
      "metadata": {
        "id": "SdXYs4Ri2eju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply preprocessing to tweets"
      ],
      "metadata": {
        "id": "46mPPPkN44g2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['tweet'] = train['tweet'].apply(lambda x: cleaning_stopwords(x))\n",
        "train['tweet'] = train['tweet'].apply(lambda x: cleaning_punctuations(x))\n",
        "train['tweet'] = train['tweet'].apply(lambda x: cleaning_repeating_char(x))\n",
        "train['tweet'] = train['tweet'].apply(lambda x: cleaning_URLs(x))\n",
        "train['tweet'] = train['tweet'].apply(lambda x: cleaning_numbers(x))"
      ],
      "metadata": {
        "id": "qB2z5LE62fb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "1h7suJPm5OIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "kc69kzT17y1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kotsNRci2iv3",
        "outputId": "c7063ea6-3a2d-434d-a83d-0c9c500f69e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = nltk.WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "wKa3C_5w2l61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": nltk.corpus.wordnet.ADJ, \"N\": nltk.corpus.wordnet.NOUN, \"V\": nltk.corpus.wordnet.VERB, \"R\": nltk.corpus.wordnet.ADV}\n",
        "    return tag_dict.get(tag, nltk.corpus.wordnet.NOUN)"
      ],
      "metadata": {
        "id": "TObGGreg2qc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_words(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
        "    return ' '.join(lemmatized_words)"
      ],
      "metadata": {
        "id": "qhpeTiK-2ymQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['lemmatized_tweet'] = train['tweet'].apply(lemmatize_words)"
      ],
      "metadata": {
        "id": "g1d964gA2-jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define preprocessor and pipeline"
      ],
      "metadata": {
        "id": "pgJTZyXx5KGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "gWoKVe_m32VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('text', TfidfVectorizer(stop_words='english', max_features=10000), 'lemmatized_tweet')\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=1000))\n",
        "])"
      ],
      "metadata": {
        "id": "0yPA2vTG3BvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data"
      ],
      "metadata": {
        "id": "izj8YPbU5bcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "TGGyRH0N38W6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train[['lemmatized_tweet']]\n",
        "y = train['disaster']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=2)"
      ],
      "metadata": {
        "id": "3uZWD77T3Fro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "_QZT3ANc5jYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "6j-HqxDA3QjO",
        "outputId": "43d6e1d4-7c11-4cbb-ea97-70d36703fb6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('preprocessor',\n",
              "                 ColumnTransformer(remainder='passthrough',\n",
              "                                   transformers=[('text',\n",
              "                                                  TfidfVectorizer(max_features=10000,\n",
              "                                                                  stop_words='english'),\n",
              "                                                  'lemmatized_tweet')])),\n",
              "                ('classifier', LogisticRegression(max_iter=1000))])"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
              "                                   transformers=[(&#x27;text&#x27;,\n",
              "                                                  TfidfVectorizer(max_features=10000,\n",
              "                                                                  stop_words=&#x27;english&#x27;),\n",
              "                                                  &#x27;lemmatized_tweet&#x27;)])),\n",
              "                (&#x27;classifier&#x27;, LogisticRegression(max_iter=1000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
              "                                   transformers=[(&#x27;text&#x27;,\n",
              "                                                  TfidfVectorizer(max_features=10000,\n",
              "                                                                  stop_words=&#x27;english&#x27;),\n",
              "                                                  &#x27;lemmatized_tweet&#x27;)])),\n",
              "                (&#x27;classifier&#x27;, LogisticRegression(max_iter=1000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
              "                  transformers=[(&#x27;text&#x27;,\n",
              "                                 TfidfVectorizer(max_features=10000,\n",
              "                                                 stop_words=&#x27;english&#x27;),\n",
              "                                 &#x27;lemmatized_tweet&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">text</label><div class=\"sk-toggleable__content\"><pre>lemmatized_tweet</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=10000, stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre>[]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "IqzqfwVk3_J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model"
      ],
      "metadata": {
        "id": "qRuig0IS5kd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pred = pipeline.predict(X_train)\n",
        "training_data_accuracy = accuracy_score(y_train, X_train_pred)\n",
        "print(\"Training Accuracy:\", training_data_accuracy)\n",
        "\n",
        "X_test_pred = pipeline.predict(X_test)\n",
        "testing_data_accuracy = accuracy_score(y_test, X_test_pred)\n",
        "print(\"Testing Accuracy:\", testing_data_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPcGG0y63YZZ",
        "outputId": "672812ad-3ce6-404f-e355-66663a4fd7e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.8827586206896552\n",
            "Testing Accuracy: 0.8069599474720945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the model"
      ],
      "metadata": {
        "id": "WJBc-vUQ5owL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "sOeyRM9w8IJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('disaster_prediction_model.pkl', 'wb') as file:\n",
        "    pickle.dump(pipeline, file)"
      ],
      "metadata": {
        "id": "ABNs-BRn3cAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model"
      ],
      "metadata": {
        "id": "HKxWjVg85poh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "with open('disaster_prediction_model.pkl', 'rb') as file:\n",
        "    best_model = pickle.load(file)"
      ],
      "metadata": {
        "id": "knzP1-aL3eZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Test data"
      ],
      "metadata": {
        "id": "6zwaK9ri7RUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('/content/drive/MyDrive/test.csv')"
      ],
      "metadata": {
        "id": "8hEIhhkt4M_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying the same preprocessing steps to the test data"
      ],
      "metadata": {
        "id": "tnIJccX87Yyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test['keyword'].fillna('missing_keyword', inplace=True)\n",
        "test['place'].fillna('unknown', inplace=True)\n",
        "test['tweet'] = test['tweet'].apply(lambda x: cleaning_stopwords(x))\n",
        "test['tweet'] = test['tweet'].apply(lambda x: cleaning_punctuations(x))\n",
        "test['tweet'] = test['tweet'].apply(lambda x: cleaning_repeating_char(x))\n",
        "test['tweet'] = test['tweet'].apply(lambda x: cleaning_URLs(x))\n",
        "test['tweet'] = test['tweet'].apply(lambda x: cleaning_numbers(x))\n",
        "test['lemmatized_tweet'] = test['tweet'].apply(lemmatize_words)"
      ],
      "metadata": {
        "id": "mxevhi-y4EaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make predictions"
      ],
      "metadata": {
        "id": "hCTiQ0OG7gNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = best_model.predict(test[['lemmatized_tweet']])"
      ],
      "metadata": {
        "id": "NisXp3GH4SeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add predictions to the DataFrame and save to a new CSV"
      ],
      "metadata": {
        "id": "cTyTvRWN7hQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test['disaster'] = predictions\n",
        "test.to_csv('predicted_disasters.csv', index=False)\n",
        "\n",
        "print(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kor8mCx24TPT",
        "outputId": "ccf0f758-3b64-4d56-d3c2-72d3e657cc89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         id          keyword    place  \\\n",
            "0         0  missing_keyword  unknown   \n",
            "1         2  missing_keyword  unknown   \n",
            "2         3  missing_keyword  unknown   \n",
            "3         9  missing_keyword  unknown   \n",
            "4        11  missing_keyword  unknown   \n",
            "...     ...              ...      ...   \n",
            "3258  10861  missing_keyword  unknown   \n",
            "3259  10865  missing_keyword  unknown   \n",
            "3260  10868  missing_keyword  unknown   \n",
            "3261  10874  missing_keyword  unknown   \n",
            "3262  10875  missing_keyword  unknown   \n",
            "\n",
            "                                                  tweet  \\\n",
            "0                        Just hapened terible car crash   \n",
            "1     Heard earthquake diferent cities stay safe eve...   \n",
            "2     forest fire spot pond gese fleing acros stret ...   \n",
            "3                 Apocalypse lighting Spokane wildfires   \n",
            "4                    Typhon Soudelor kils  China Taiwan   \n",
            "...                                                 ...   \n",
            "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...   \n",
            "3259  Storm RI worse last huricane My cityampothers ...   \n",
            "3260      Gren Line derailment Chicago htptcoUtbXLcBIuY   \n",
            "3261  MEG isues Hazardous Weather Outlok HWO htptcoX...   \n",
            "3262  CityofCalgary activated Municipal Emergency Pl...   \n",
            "\n",
            "                                       lemmatized_tweet  disaster  \n",
            "0                        Just hapened terible car crash         1  \n",
            "1     Heard earthquake diferent city stay safe everyone         1  \n",
            "2     forest fire spot pond gese flee acros stret I ...         1  \n",
            "3                     Apocalypse light Spokane wildfire         1  \n",
            "4                     Typhon Soudelor kils China Taiwan         1  \n",
            "...                                                 ...       ...  \n",
            "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...         1  \n",
            "3259  Storm RI bad last huricane My cityampothers ha...         1  \n",
            "3260      Gren Line derailment Chicago htptcoUtbXLcBIuY         1  \n",
            "3261  MEG isues Hazardous Weather Outlok HWO htptcoX...         1  \n",
            "3262  CityofCalgary activate Municipal Emergency Pla...         0  \n",
            "\n",
            "[3263 rows x 6 columns]\n"
          ]
        }
      ]
    }
  ]
}